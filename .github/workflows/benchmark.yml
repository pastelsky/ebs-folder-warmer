name: Performance Benchmarks

on:
  pull_request:
    branches: [ main ]
    paths:
      - 'disk-warmer/**'
      - 'bench/**'
      - '.github/workflows/benchmark.yml'
  push:
    branches: [ main ]
    paths:
      - 'disk-warmer/**'
      - 'bench/**'
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Benchmark type to run'
        required: false
        default: 'quick'
        type: choice
        options:
        - quick
        - full
        - effectiveness

permissions:
  contents: read
  pull-requests: write
  checks: write

jobs:
  benchmark:
    name: Run Performance Benchmarks
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential libaio-dev hyperfine fio

    - name: Build disk-warmer
      run: |
        cd disk-warmer
        make clean && make
        ./disk-warmer --version

    - name: Create virtual disk for testing
      run: |
        # Create a 4GB virtual disk for testing
        sudo mkdir -p /tmp/benchmark-disk
        sudo fallocate -l 4G /tmp/benchmark-disk/virtual-disk.img
        
        # Setup loop device
        sudo losetup -P /dev/loop7 /tmp/benchmark-disk/virtual-disk.img || true
        sudo mkfs.ext4 /dev/loop7
        
        # Mount the virtual disk
        sudo mkdir -p /mnt/benchmark
        sudo mount /dev/loop7 /mnt/benchmark
        sudo chmod 777 /mnt/benchmark
        
        echo "BENCHMARK_DEVICE=/dev/loop7" >> $GITHUB_ENV
        echo "TEST_DIR=/mnt/benchmark/test-data" >> $GITHUB_ENV

    - name: Run quick benchmarks
      if: github.event_name == 'pull_request' || github.event.inputs.benchmark_type == 'quick'
      run: |
        cd bench
        ./benchmark.sh -d $BENCHMARK_DEVICE directory configuration
        
    - name: Run effectiveness benchmarks  
      if: github.event_name == 'push' || github.event.inputs.benchmark_type == 'effectiveness'
      run: |
        cd bench
        ./benchmark.sh -d $BENCHMARK_DEVICE effectiveness

    - name: Run full benchmarks
      if: github.event.inputs.benchmark_type == 'full'
      run: |
        cd bench
        ./benchmark.sh -d $BENCHMARK_DEVICE all

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ github.sha }}
        path: bench/results/
        retention-days: 30

    - name: Comment benchmark results on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const path = require('path');
          
          // Read benchmark report
          const resultsDir = 'bench/results';
          const reportPath = path.join(resultsDir, 'benchmark_report.md');
          
          if (!fs.existsSync(reportPath)) {
            console.log('No benchmark report found');
            return;
          }
          
          const benchmarkReport = fs.readFileSync(reportPath, 'utf8');
          
          // Create comment body
          const comment = `## ðŸš€ Performance Benchmark Results
          
          Benchmarks were run for this PR to measure performance impact.
          
          <details>
          <summary>ðŸ“Š Detailed Results</summary>
          
          ${benchmarkReport}
          
          </details>
          
          ðŸ“Ž Full results are available in the [artifacts](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}).
          
          ---
          *Benchmarks run on commit ${context.sha.substring(0, 7)}*`;
          
          // Post comment
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });

    - name: Generate performance comparison
      if: github.event_name == 'pull_request'
      run: |
        cd bench
        
        # Create a summary of key metrics
        echo "## Performance Summary" > results/performance_summary.md
        echo "" >> results/performance_summary.md
        
        # Extract timing information from JSON results
        if [ -f "results/directory_warming.json" ]; then
          echo "### Directory Warming Performance" >> results/performance_summary.md
          echo "" >> results/performance_summary.md
          
          # Parse hyperfine JSON and extract key metrics
          python3 -c "
          import json
          import sys
          
          try:
              with open('results/directory_warming.json', 'r') as f:
                  data = json.load(f)
              
              print('| Test | Mean Time | Min Time | Max Time |')
              print('|------|-----------|----------|----------|')
              
              for result in data.get('results', []):
                  command = result.get('command', 'Unknown')
                  mean = result.get('mean', 0)
                  min_time = result.get('min', 0)
                  max_time = result.get('max', 0)
                  
                  # Simplify command name
                  if 'db' in command:
                      test_name = 'Database Files'
                  elif 'logs' in command:
                      test_name = 'Log Files'
                  elif 'config' in command:
                      test_name = 'Config Files'
                  elif 'web' in command:
                      test_name = 'Web Content'
                  else:
                      test_name = 'Unknown'
                  
                  print(f'| {test_name} | {mean:.2f}s | {min_time:.2f}s | {max_time:.2f}s |')
                  
          except Exception as e:
              print(f'Error parsing results: {e}')
          " >> results/performance_summary.md
          
          echo "" >> results/performance_summary.md
        fi
        
        # Add disk usage information
        echo "### Test Environment" >> results/performance_summary.md
        echo "" >> results/performance_summary.md
        echo "- **Test Device**: $BENCHMARK_DEVICE" >> results/performance_summary.md
        echo "- **Test Directory**: $TEST_DIR" >> results/performance_summary.md
        echo "- **Disk Warmer Version**: $(./disk-warmer --version)" >> results/performance_summary.md
        echo "- **System**: $(uname -a)" >> results/performance_summary.md

    - name: Cleanup virtual disk
      if: always()
      run: |
        sudo umount /mnt/benchmark || true
        sudo losetup -d /dev/loop7 || true
        sudo rm -rf /tmp/benchmark-disk || true

  benchmark-comparison:
    name: Compare with Baseline
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    needs: benchmark
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Checkout main branch for baseline
      run: |
        git fetch origin main
        git checkout origin/main
        
    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential libaio-dev hyperfine fio

    - name: Build baseline disk-warmer
      run: |
        cd disk-warmer
        make clean && make
        cp disk-warmer disk-warmer-baseline

    - name: Switch back to PR branch
      run: |
        git checkout ${{ github.head_ref }}

    - name: Build PR disk-warmer
      run: |
        cd disk-warmer  
        make clean && make
        cp disk-warmer disk-warmer-pr

    - name: Run comparison benchmark
      run: |
        # Create test environment
        sudo mkdir -p /tmp/comparison-test
        sudo fallocate -l 2G /tmp/comparison-test/virtual-disk.img
        sudo losetup -P /dev/loop8 /tmp/comparison-test/virtual-disk.img
        sudo mkfs.ext4 /dev/loop8
        sudo mkdir -p /mnt/comparison
        sudo mount /dev/loop8 /mnt/comparison
        sudo chmod 777 /mnt/comparison
        
        # Create test data
        mkdir -p /mnt/comparison/test-data
        dd if=/dev/urandom of=/mnt/comparison/test-data/test.dat bs=1M count=512
        
        # Compare performance
        hyperfine \
          --warmup 1 \
          --min-runs 3 \
          --prepare 'sync; sudo sh -c "echo 3 > /proc/sys/vm/drop_caches"' \
          --export-markdown comparison_results.md \
          --command-name "Baseline (main)" "sudo disk-warmer/disk-warmer-baseline /mnt/comparison/test-data /dev/loop8" \
          --command-name "PR Changes" "sudo disk-warmer/disk-warmer-pr /mnt/comparison/test-data /dev/loop8"
        
        # Cleanup
        sudo umount /mnt/comparison
        sudo losetup -d /dev/loop8
        sudo rm -rf /tmp/comparison-test

    - name: Comment comparison results
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          
          if (!fs.existsSync('comparison_results.md')) {
            console.log('No comparison results found');
            return;
          }
          
          const comparisonResults = fs.readFileSync('comparison_results.md', 'utf8');
          
          const comment = `## âš¡ Performance Comparison
          
          Comparison between main branch (baseline) and this PR:
          
          ${comparisonResults}
          
          > This comparison uses a controlled virtual disk environment. Real-world performance may vary based on hardware and workload patterns.`;
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          }); 